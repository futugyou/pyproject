{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/futugyou/pyproject/blob/master/google_colab/TextClusteringAndTopicModeling_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_BCbm4KCjL6"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install --upgrade pip setuptools wheel\n",
    "%pip install datasets\n",
    "%pip install sentence_transformers\n",
    "%pip install umap-learn\n",
    "%pip install hdbscan\n",
    "%pip install numpy\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install tqdm\n",
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zQ-uwrRkkBi"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from tqdm.notebook import tqdm\n",
    "import umap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = load_dataset(\"maartengr/arxiv_nlp\")\n",
    "dataset = data[\"train\"]\n",
    "abstracts = dataset[\"Abstracts\"]\n",
    "titles = dataset[\"Titles\"]\n",
    "print(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0y-DR6nKjQlQ"
   },
   "outputs": [],
   "source": [
    "# \"thenlper/gte-small\" \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_model = SentenceTransformer(\"thenlper/gte-small\")\n",
    "embeddings = embedding_model.encode(\n",
    "    abstracts, show_progress_bar=True, progress_bar=tqdm, batch_size=16\n",
    ")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC-1yYJmAnvu"
   },
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(\n",
    "    n_components=5, min_dist=0.0, metric=\"cosine\", random_state=42, verbose=True\n",
    ")\n",
    "umap_embeddings = umap_model.fit_transform(embeddings)\n",
    "print(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhdFBvBon_rN"
   },
   "outputs": [],
   "source": [
    "hdbsacn_model = HDBSCAN(\n",
    "    min_cluster_size=50, metric=\"euclidean\", cluster_selection_method=\"eom\"\n",
    ").fit(umap_embeddings)\n",
    "clusters = hdbsacn_model.labels_\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bf3b1qnXTVYG"
   },
   "outputs": [],
   "source": [
    "reduced_embeddings = umap.UMAP(\n",
    "    n_components=2, min_dist=0.0, metric=\"cosine\", random_state=42, verbose=True\n",
    ").fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeLlj-uFsMYJ"
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=umap_model,  # option\n",
    "    hdbscan_model=hdbsacn_model,  # option\n",
    "    verbose=True,\n",
    ").fit(abstracts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfZiz23py12g"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "original_topics = deepcopy(topic_model.topic_representations_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x6kJBAga0khK"
   },
   "outputs": [],
   "source": [
    "def topic_differences(model, original_topics, nr_topics=5):\n",
    "    df = pd.DataFrame(columns=[\"topic\", \"original\", \"updated\"])\n",
    "    for topic in range(nr_topics):\n",
    "        og_words = \" | \".join(list(zip(*original_topics[topic]))[0][:5])\n",
    "        new_words = \" | \".join(list(zip(*model.get_topic(topic)))[0][:5])\n",
    "        df.loc[len(df)] = [topic, og_words, new_words]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHciUHmS56Yh"
   },
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bk29y1Fm5_yz"
   },
   "outputs": [],
   "source": [
    "from bertopic.representation import TextGeneration\n",
    "from transformers import pipeline\n",
    "\n",
    "prompt = \"\"\"i have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "the topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "base on the documents and keywords, what is this topic about?\n",
    "\"\"\"\n",
    "\n",
    "gerenator = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\")\n",
    "representation_model = TextGeneration(\n",
    "    gerenator, prompt=prompt, doc_length=50, tokenizer=\"whitespace\"\n",
    ")\n",
    "\n",
    "topic_model.update_topics(abstracts, representation_model=representation_model)\n",
    "topic_differences(topic_model, original_topics)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}