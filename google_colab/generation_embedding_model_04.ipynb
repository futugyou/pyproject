{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/futugyou/pyproject/blob/master/google_colab/generation_embedding_model_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "1piQHEu3zW6s"
   },
   "outputs": [],
   "source": [
    "# Install required dependencies\n",
    "%pip install datasets\n",
    "%pip install sentence_transformers\n",
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install tqdm\n",
    "%pip install scikit-learn\n",
    "%pip install mteb\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjS_tUJz6Kp-"
   },
   "outputs": [],
   "source": [
    "# The code of generate-modle is almost identical to finetune-model\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from datasets import load_dataset, Dataset\n",
    "from sentence_transformers import InputExample\n",
    "from sentence_transformers.dataset import NoDuplicatesDataLoader\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000))\n",
    "mapping = {2: 0, 1: 0, 0: 1}\n",
    "gold_examples = [\n",
    "    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=mapping[row[\"label\"]])\n",
    "    for row in tqdm(dataset)\n",
    "]\n",
    "gold_dataloader = NoDuplicatesDataLoader(gold_examples, batch_size=32)\n",
    "gold = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence1\": dataset[\"premise\"],\n",
    "        \"sentence2\": dataset[\"hypothesis\"],\n",
    "        \"label\": [mapping[label] for label in dataset[\"label\"]],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgxgBEHbD4UO"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(\"bert-base-uncased\", num_labels=2)\n",
    "cross_encoder.fit(\n",
    "    train_dataloader=gold_dataloader,\n",
    "    epochs=1,\n",
    "    show_progress_bar=True,\n",
    "    warmup_steps=100,\n",
    "    use_amp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silver = load_dataset(\"glue\", \"mnli\", split=\"train\").select(range(10_000, 50_000))\n",
    "pairs = list(zip(silver[\"premise\"], silver[\"hypothesis\"]))\n",
    "\n",
    "output = cross_encoder.predict(pairs, apply_softmax=True, show_progress_bar=True)\n",
    "silver = pd.DataFrame(\n",
    "    {\n",
    "        \"sentence1\": silver[\"premise\"],\n",
    "        \"sentence2\": silver[\"hypothesis\"],\n",
    "        \"label\": np.argmax(output, axis=1),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data, data2], ignore_index=True, axis=0)\n",
    "data = data.drop_duplicates(subset=[\"sentence1\", \"sentence2\"], keep=\"first\")\n",
    "train_dataset = Dataset.from_pandas(data, preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\n",
    "val_sts = load_dataset(\"glue\", \"stsb\", split=\"validation\")\n",
    "evaluator = EmbeddingSimilarityEvaluator(\n",
    "    sentences1=val_sts[\"sentence1\"],\n",
    "    sentences2=val_sts[\"sentence2\"],\n",
    "    scores=[score / 5 for score in val_sts[\"label\"]],\n",
    "    main_similarity=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "\n",
    "# model\n",
    "mebedding_model = SentenceTransformer(\"bert-base-uncased\")\n",
    "\n",
    "# loss function\n",
    "train_loss = losses.CosineSimilarityLoss(model=mebedding_model)\n",
    "\n",
    "# training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"augmented_mebedding_model\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=100,\n",
    "    fp16=True,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=embedding_model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator model\n",
    "evaluator(embedding_model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
